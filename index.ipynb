{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chaitanya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import csv\n",
    "from PyPDF2 import PdfReader\n",
    "nltk.download('stopwords')\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "import re\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(path, codec='utf-8'):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "    \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [re.sub(r'[^\\w\\s]', '', w) for w in word_tokens ]\n",
    "    \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    \n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    \n",
    "    found_skills = set()\n",
    "    \n",
    "\n",
    "    with open('skills.csv', newline='') as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "\n",
    "    #print(data[0])\n",
    "\n",
    "    #arr = np.loadtxt(\"skills.csv\",delimiter=\",\", dtype=str)\n",
    "    skilldataset = [w.lower() for w in data[0]]\n",
    "    \n",
    "    #print(skilldataset)\n",
    "    skilldataset = [re.sub(r'[^\\w\\s]', '', w) for w in skilldataset]\n",
    "    # we search for each token in our skills database\n",
    "    #print(skilldataset)\n",
    "    for token in filtered_tokens:\n",
    "    \n",
    "        if token.lower() in skilldataset:\n",
    "            \n",
    "            found_skills.add(token)\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    \n",
    "    for ngram in bigrams_trigrams:\n",
    "        \n",
    "        if ngram.lower() in skilldataset:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Status', 'SQL', 'JSP', 'C', 'ENGINEERING', 'Putty', 'Visual Studio Code', ' Beyond Compare', 'analytical', 'Servers', 'Eclipse', 'Unix', 'Oracle', 'Hardware', 'technical', 'Java', 'process improvement', 'process', 'improvement', 'controlM', 'JAVA', 'system', 'Email', 'communication', 'PLSQL', 'JavaScript', 'security', 'SDLC', 'email', 'WINSCP', 'Windows', 'Technical', 'PHP', 'Technical Skills', 'testing', 'analysis', 'design', 'analyze', 'AspNet', 'Visual', 'UNIX'}\n"
     ]
    }
   ],
   "source": [
    "#pdf_path = '/home/chaitanya/Public/autosure/parsing/testcv.pdf'\n",
    "#text1=extract_text_from_pdf(pdf_path)\n",
    "text = extract_text_from_docx('/home/chaitanya/Public/autosure/parsing/CVs/Profiles of September/KavyaSharma (1).docx')\n",
    "skills = extract_skills(text)\n",
    "print(skills)\n",
    "text1 = extract_text_from_pdf('/home/chaitanya/Public/autosure/parsing/testcv.pdf')\n",
    "skills1 = extract_skills(text1)\n",
    "print(skills1)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'... some string with! punctuation ...'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('resumevnev': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e5f46bbf9534cabbc0e69fe64b3d813409bc6cd98ace6c51412473cd36281a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
